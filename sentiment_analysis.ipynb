{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f71139d-16ef-4720-885b-c8c62e302b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb09e81b-17d3-48e8-b1bd-4a74155af2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1566931-66c5-4644-b625-ae5db533b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83eb3ac8-3f67-4bca-85be-298a89a6b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of stopwords 420\n"
     ]
    }
   ],
   "source": [
    "stopwords = set()\n",
    "with open(os.path.join('data', 'stopwords.txt')) as f :  \n",
    "    for line in f :\n",
    "        stopwords.add(line.strip())\n",
    "print('Length of stopwords',len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be99d4a7-ebe1-46d0-b857-69264cf29708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s) :\n",
    "    s = s.lower()\n",
    "    s = nltk.tokenize.word_tokenize(s)\n",
    "    s = [token for token in s if len(token) > 2]\n",
    "    s = [wordnet_lemmatizer.lemmatize(token) for token in s]\n",
    "    s = [token for token in s if token not in stopwords]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b71741c7-d80d-4d1b-86a8-615dbc7acd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of postive reviews : 1000\n",
      "Length of negative reviews : 1000\n"
     ]
    }
   ],
   "source": [
    "positive_reviews = BeautifulSoup(open(os.path.join('data', 'positive.review')))\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open(os.path.join('data', 'negative.review')))\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "print(f'Length of postive reviews : {len(positive_reviews)}\\nLength of negative reviews : {len(negative_reviews)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614e6c13-2f00-4afb-9168-64d1180a2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "postive_tokenized = []\n",
    "negative_tokenized = []\n",
    "i = 0\n",
    "\n",
    "for p in positive_reviews :\n",
    "    tokenized = tokenize(p.text)\n",
    "    postive_tokenized.append(tokenized)\n",
    "    for token in tokenized :\n",
    "        if token not in word_index :\n",
    "            word_index[token] = i\n",
    "            i += 1\n",
    "            \n",
    "for n in negative_reviews :\n",
    "    tokenized = tokenize(n.text)\n",
    "    negative_tokenized.append(tokenized)\n",
    "    for token in tokenized :\n",
    "        if token not in word_index :\n",
    "            word_index[token] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "174947f0-390f-4607-9167-1368c65f0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10948)\n"
     ]
    }
   ],
   "source": [
    "positive_vectorized = np.zeros((len(postive_tokenized), len(word_index)))\n",
    "negative_vectorized = np.zeros((len(negative_tokenized), len(word_index)))\n",
    "\n",
    "for idx, p in enumerate(postive_tokenized) :\n",
    "    for word in p :\n",
    "        positive_vectorized[idx, word_index[word]] += 1\n",
    "        break\n",
    "\n",
    "for idx, n in enumerate(negative_tokenized) :\n",
    "    for word in n :\n",
    "        negative_vectorized[idx, word_index[word]] += 1\n",
    "\n",
    "data = np.vstack([\n",
    "    positive_vectorized,\n",
    "    negative_vectorized\n",
    "])\n",
    "\n",
    "\n",
    "data = data / data.sum(axis = 1, keepdims = True)\n",
    "data = np.concatenate([\n",
    "    data,\n",
    "    np.vstack((np.ones(shape=(positive_vectorized.shape[0], 1)), np.zeros(shape=(positive_vectorized.shape[0], 1))))\n",
    "], axis = 1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23c751d1-1ce2-4985-aeb8-3cd1c8c7bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31917eb9-2999-4aa7-a438-c3cb69155bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data[:-100,:-1], data[-100:,:-1], data[:-100,-1], data[-100:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a171d223-7b05-4733-b44c-dfa55d290b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression = LogisticRegression()\n",
    "regression.fit(X_train, y_train)\n",
    "regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98471438-21a5-48c5-bd26-20a6472c6686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f90bac74-d9ad-4ebb-a532-b86b403cffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchased 2.4095278956410526\n",
      "unit 0.9621390836337539\n",
      "day -1.2778596848106434\n",
      "'ve 2.420227820043411\n",
      "month -0.9757694465872293\n",
      "pro 1.521876623237843\n",
      "simple 1.5076418531325149\n",
      "sound 1.1552669151413373\n",
      "lot 1.1756754340759135\n",
      "easy 1.4423939875077614\n",
      "card 0.9274843814874687\n",
      "recently 2.0430073331236946\n",
      "product 1.5338585442401902\n",
      "wa 1.4594995462163982\n",
      "perfect 1.0112816726973073\n",
      "money -0.9926794108197323\n",
      "review 1.1847095387141227\n",
      "bought 2.8184873424555072\n",
      "canon 1.314237742380012\n",
      "read 1.0686542463915658\n",
      "happy 1.6088703078053095\n",
      "device 0.9039359343374037\n",
      "note 1.003626466377106\n",
      "pleased 1.5201338266875166\n",
      "sandisk 1.3665461244847865\n",
      "earphone 1.2282702910643308\n",
      "support -1.025166598570388\n",
      "little 1.6203760366572217\n",
      "excellent 1.9646653679539872\n",
      "love 1.8995315727169282\n",
      "found 1.4011457925806983\n",
      "keyboard 1.1457816575693827\n",
      "using 1.7457138918495538\n",
      "logitech 0.9375487526831355\n",
      "mini 0.993497078635716\n",
      "nice 0.9626599208392497\n",
      "then -2.0467166184226318\n",
      "people 1.01015625757233\n",
      "printer 1.1272800986050857\n",
      "hour -0.9149269442939243\n",
      "headphone 1.1428026780665659\n",
      "backpack 1.0791969687825271\n",
      "decided 0.9287841764918588\n",
      "speaker 1.4659629336522964\n",
      "received 1.1292067280253917\n",
      "wonderful 1.2542445326821052\n",
      "overall 1.173804417414435\n",
      "satisfied 1.0597055838440432\n",
      "previously 1.2524368792199951\n",
      "glad 1.0598968859612417\n",
      "ink 0.9985208813773221\n",
      "wow 1.051957064787876\n",
      "gps 0.9151640619081968\n",
      "return -1.053803819456829\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "for word, index in word_index.items():\n",
    "    weight = regression.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6ac67-3fc2-4819-b3fa-6c6d3fba62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ad5d8-ebbe-4f6c-8a68-de97b219ebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
