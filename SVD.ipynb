{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6347c920-88af-4639-97ef-8830039f03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1713eedd-de49-490c-9e4e-67069bbed5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373 titles found.\n",
      "420 stopwords found.\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "with open(os.path.join('data', 'all_book_titles.txt'), encoding = 'utf-8') as f :\n",
    "    for line in f :\n",
    "        line = line.strip().lower()\n",
    "        if line:\n",
    "            titles.append(line)\n",
    "            \n",
    "stopwords = set()\n",
    "with open(os.path.join('data', 'stopwords.txt'), encoding = 'utf-8') as f :\n",
    "    for line in f :\n",
    "        line = line.strip().lower()\n",
    "        if line:\n",
    "            stopwords.add(line)\n",
    "\n",
    "stopwords.union({'i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "                 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "                 \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "                 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "                 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', \n",
    "                 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "                 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \n",
    "                 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "                 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', \n",
    "                 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', \n",
    "                 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "                 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "                 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "                 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
    "                 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n",
    "                 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \n",
    "                 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "                 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "                 \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \n",
    "                 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \n",
    "                 \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "                 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", \n",
    "                 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \n",
    "                 \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                 'wouldn', \"wouldn't\", \"edition\", \"series\", \"introduction\", \"application\"\n",
    "                \"first\", 'second', 'third', 'fourth', 'guide', 'brief'})\n",
    "            \n",
    "print(f'{len(titles)} titles found.\\n{len(stopwords)} stopwords found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6cc7141-a1a4-444d-9042-493067fd86ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens : 1978\n"
     ]
    }
   ],
   "source": [
    "def my_tokenize_function(text):\n",
    "    token_list = nltk.tokenize.word_tokenize(text)\n",
    "    token_list = [word_net_lemmatizer.lemmatize(token) for token in token_list \n",
    "                  if len(token) > 2 \n",
    "                  and token.isalpha() \n",
    "                  and token not in stopwords]\n",
    "    return token_list\n",
    "\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = Tokenizer(analyzer=my_tokenize_function)\n",
    "tokenizer.fit_on_texts(titles)\n",
    "print(f'Unique tokens : {len(tokenizer.word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2024693e-e7de-402c-a788-971d0a57c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]] (1978, 2373)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "X = []\n",
    "for sequence in sequences :\n",
    "    counter = Counter(tokenizer.index_word.keys())\n",
    "    counter.update(sequence)\n",
    "    counter.subtract(Counter(tokenizer.index_word.keys()))\n",
    "    x = list(counter.values())\n",
    "    X.append(x)\n",
    "    \n",
    "X = np.array(X) #shape = (N, D) Document Term matrix\n",
    "X_t = X.T\n",
    "print(X_t, X_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19ef0ba0-234c-444a-81b3-e42a7307e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2373, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD()\n",
    "Z = svd.fit_transform(X)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9be0644-80e1-4da1-87c2-5b3456e9f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.scatter(Z[:, 0], Z[:, 1])\n",
    "for i in range(1, len(tokenizer.word_index)) :\n",
    "    plt.annotate(text = tokenizer.index_word[i], xy = (Z[i, 0], Z[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0a27f99-e05a-47f2-a682-f39fd383aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7800857-e1f8-4986-b0ee-7d6de332a4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
